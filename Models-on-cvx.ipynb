{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47274983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "184c43bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input\n",
    "#600x1800x1800\n",
    "#CHANGE TO TMC\n",
    "#x = np.load('xAll0.npy')\n",
    "x = np.load('x75Convex.npy')\n",
    "x1 = np.load('xAll1.npy')\n",
    "#x = np.random.normal(size = (600, 1800,1800))\n",
    "#x1 = np.random.normal(size = (100, 1800, 1800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14af7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 1800, 1800, 1)\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(-1,1800,1800, 1)\n",
    "x1 = x1.reshape(-1,1800,1800, 1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ee99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VAL = 150\n",
    "N_TRAIN = 150\n",
    "train_examples = x[:N_TRAIN,:,:,:]\n",
    "val_examples = x1[:N_VAL, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe8ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 8)\n"
     ]
    }
   ],
   "source": [
    "#y = np.load('y0.npy')\n",
    "y = np.load('y75Convex.npy')\n",
    "y1 = np.load('y1.npy')\n",
    "#y = np.random.choice([0,1], size = (600, 8))\n",
    "#y1 = np.random.choice([0,1], size = (N_VAL,8))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1a2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random NxD array (N images/vecs - 600, D diagnoses - 8)\n",
    "#train input on this\n",
    "\n",
    "#train_labels = np.random.choice([0,1], (N,D))\n",
    "#print(train_labels.shape)\n",
    "val_labels = y1[:N_VAL,:]#diag0[500:, :]\n",
    "train_labels = y[:N_TRAIN,:] #diag0[:500, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "665d3568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 <TensorSliceDataset shapes: ((1800, 1800, 1), (8,)), types: (tf.float16, tf.float64)>\n",
      "3 <BatchDataset shapes: ((None, 1800, 1800, 1), (None, 8)), types: (tf.float16, tf.float64)>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_examples, val_labels))\n",
    "print(len(train_dataset), train_dataset)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "print(len(train_dataset), train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5428832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#train_dataset = train_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "#val_dataset = val_dataset.prefetch(buffer_size = AUTOTUNE)\n",
    "#print(len(train_dataset), train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e98b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 1800, 1800, 1)]   0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 1799, 1799, 3)     15        \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo (None, 1799, 1799, 3)     6         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 899, 899, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 898, 898, 3)       39        \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer (None, 898, 898, 3)       6         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 449, 449, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 448, 448, 3)       39        \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer (None, 448, 448, 3)       6         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 223, 223, 3)       39        \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer (None, 223, 223, 3)       6         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 111, 111, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 110, 110, 3)       39        \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer (None, 110, 110, 3)       6         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 55, 55, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 54, 54, 3)         39        \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer (None, 54, 54, 3)         6         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 27, 27, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2187)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1200)              2625600   \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer (None, 1200)              2400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600)               720600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 4808      \n",
      "=================================================================\n",
      "Total params: 3,353,654\n",
      "Trainable params: 3,353,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "3/3 [==============================] - 39s 13s/step - loss: 2.6752 - accuracy: 0.1333 - val_loss: 2.7880 - val_accuracy: 0.1467\n",
      "Epoch 2/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.5379 - accuracy: 0.1333 - val_loss: 2.7290 - val_accuracy: 0.2000\n",
      "Epoch 3/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.4535 - accuracy: 0.2000 - val_loss: 2.6839 - val_accuracy: 0.2733\n",
      "Epoch 4/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.3824 - accuracy: 0.2800 - val_loss: 2.6472 - val_accuracy: 0.2867\n",
      "Epoch 5/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.3188 - accuracy: 0.3467 - val_loss: 2.6167 - val_accuracy: 0.3200\n",
      "Epoch 6/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.2614 - accuracy: 0.4267 - val_loss: 2.5876 - val_accuracy: 0.3200\n",
      "Epoch 7/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.2065 - accuracy: 0.4533 - val_loss: 2.5578 - val_accuracy: 0.3267\n",
      "Epoch 8/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.1507 - accuracy: 0.4267 - val_loss: 2.5276 - val_accuracy: 0.3400\n",
      "Epoch 9/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.0932 - accuracy: 0.4267 - val_loss: 2.4973 - val_accuracy: 0.3400\n",
      "Epoch 10/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 2.0323 - accuracy: 0.4400 - val_loss: 2.4669 - val_accuracy: 0.3333\n",
      "Epoch 11/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 1.9683 - accuracy: 0.4267 - val_loss: 2.4373 - val_accuracy: 0.3400\n",
      "Epoch 12/15\n",
      "3/3 [==============================] - 38s 13s/step - loss: 1.9036 - accuracy: 0.4267 - val_loss: 2.4095 - val_accuracy: 0.3467\n",
      "Epoch 13/15\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.8398 - accuracy: 0.3867"
     ]
    }
   ],
   "source": [
    "models = [(2, 1200, 600),(3, 1200, 600), (2, 1000, 800), (3, 1000, 800)]\n",
    "numReg = 4\n",
    "numLr = 3\n",
    "numEpochs = 15\n",
    "#models = [(3,1200, 8),...] #filter, size of FC1, size FC2\n",
    "\n",
    "#6 conv, 3 dense, use layernorms pls\n",
    "#try filter size 3 and 2 and 4\n",
    "#try different size dense layers\n",
    "input_tensor = tf.keras.Input(shape=(1800,1800,1) )\n",
    "best_acc = 0\n",
    "best_model = None\n",
    "\n",
    "\n",
    "###init\n",
    "\n",
    "reg = 7.487 #(np.random.rand()*5) + 3\n",
    "lr = 5.102 #(np.random.rand()*4) + 2\n",
    "\n",
    "#3.3701311397595526_5.2590542363191535_(2, 1000, 800).csv\n",
    "r = 10**(-7.487)\n",
    "learningRate = 10**(-5.102)\n",
    "filter_size, FC1_size, FC2_size = (2,1200,600)\n",
    "input_tensor = tf.keras.Input(shape=(1800,1800,1) )\n",
    "\n",
    "###set up model\n",
    "##CNNs & layer norms\n",
    "#strides - default(1)\n",
    "c1 = Conv2D(3, kernel_size=(filter_size, filter_size), activation='relu', \n",
    "            input_shape=(1800,1800,1), kernel_regularizer = tf.keras.regularizers.L2(r))(input_tensor) \n",
    "l1 = tf.keras.layers.LayerNormalization()(c1)\n",
    "mp1 = tf.keras.layers.MaxPool2D()(l1)\n",
    "\n",
    "c2 = Conv2D(3, kernel_size=(filter_size, filter_size), activation='relu', \n",
    "            kernel_regularizer = tf.keras.regularizers.L2(r))(mp1)\n",
    "l2 = tf.keras.layers.LayerNormalization()(c2)\n",
    "mp2 = tf.keras.layers.MaxPool2D()(l2)\n",
    "\n",
    "c3 = Conv2D(3, kernel_size=(filter_size, filter_size), activation='relu', \n",
    "            kernel_regularizer = tf.keras.regularizers.L2(r))(mp2)\n",
    "l3 = tf.keras.layers.LayerNormalization()(c3)\n",
    "mp3 = tf.keras.layers.MaxPool2D()(l3)\n",
    "\n",
    "c4 = Conv2D(3, kernel_size=(filter_size, filter_size), activation='relu', \n",
    "kernel_regularizer = tf.keras.regularizers.L2(r))(mp3)\n",
    "l4 = tf.keras.layers.LayerNormalization()(c4)\n",
    "mp4 = tf.keras.layers.MaxPool2D()(l4)\n",
    "\n",
    "c5 = Conv2D(3, kernel_size=(filter_size, filter_size), activation='relu', \n",
    "kernel_regularizer = tf.keras.regularizers.L2(r))(mp4)\n",
    "l5 = tf.keras.layers.LayerNormalization()(c5)\n",
    "mp5 = tf.keras.layers.MaxPool2D()(l5)\n",
    "\n",
    "c6 = Conv2D(3, kernel_size=(filter_size, filter_size), activation='relu', \n",
    "kernel_regularizer = tf.keras.regularizers.L2(r))(mp5)\n",
    "l6 = tf.keras.layers.LayerNormalization()(c6)\n",
    "mp6 = tf.keras.layers.MaxPool2D()(l6)\n",
    "\n",
    "##FCs\n",
    "head_out = Flatten()(mp6)\n",
    "\n",
    "FC1 = Dense(FC1_size, activation = 'relu', kernel_regularizer = tf.keras.regularizers.L2(r))(head_out)\n",
    "l7 = tf.keras.layers.LayerNormalization()(FC1)\n",
    "FC2 = Dense(FC2_size, activation = 'relu', kernel_regularizer = tf.keras.regularizers.L2(r))(l7)\n",
    "predictions = Dense(8, activation = 'sigmoid', kernel_regularizer = tf.keras.regularizers.L2(r)) (FC2)\n",
    "\n",
    "#connect model\n",
    "model = Model(inputs=input_tensor,outputs=predictions)\n",
    "#run\n",
    "model.compile(loss= categorical_crossentropy,\n",
    "              optimizer=Adam(learning_rate=learningRate),\n",
    "              metrics=['accuracy'])\n",
    "#runs\n",
    "model.summary()\n",
    "history = model.fit(train_dataset, validation_data = val_dataset, epochs = numEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae418cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "fname = 'data_selection/conv_tmc_convex/'+str(reg)+'_'+str(lr)+'_(2,1200,600)_' + '15_epochs'\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist.to_csv(fname+'.csv')\n",
    "\n",
    "model.save('data_selection/conv_tmc_convex/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26490982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "N_TEST = 150\n",
    "test_examples = x1[-N_TEST:, :, :, :]\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_examples.shape)\n",
    "print(len(test_dataset), test_dataset)\n",
    "#Batch = 32 ? rn its 64\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "print(len(test_dataset), test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f840a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)\n",
    "print(predictions.shape)\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6987d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save test\n",
    "fname = 'data_selection/conv_tmc_convex/preds_'+str(reg)+'_'+str(lr)+'_(2,1200,600)_' + '15_epochs'\n",
    "pred = pd.DataFrame(predictions)\n",
    "pred.to_csv(fname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5906952",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lded = pd.read_csv(fname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596271c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dc750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data to model\n",
    "#history = model.fit(train_dataset, validation_data = val_dataset, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f129ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,15))\n",
    "axs[0].plot(history.history['loss'])\n",
    "axs[0].plot(history.history['val_loss'])\n",
    "axs[0].title.set_text('Training Loss vs Validation Loss')\n",
    "axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "axs[0].legend(['Train','Val'])\n",
    "axs[1].plot(history.history['accuracy'])\n",
    "axs[1].plot(history.history['val_accuracy'])\n",
    "axs[1].title.set_text('Training Accuracy vs Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Accuracy')\n",
    "axs[1].legend(['Train', 'Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25d774",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
